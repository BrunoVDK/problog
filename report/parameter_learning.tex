\fakesection{Parameter Learning}

%%%
%%%
%%%

Learning from interpretations can be done by compiling these interpretations together with the base program into some kind of structure such that inference becomes tractable. In the algorithm that was written, a d-DNNF is generated for each interpretation (an SDD in particular since SharpSAT had trouble with a known issue). Then, the weights of the parameters of interest are updated iteratively until convergence i.e. until new weights don't differ much from the previous ones. The resulting (local) optimum approximates the parameter's real values.\\

\par\noindent The d-DNNFs are generated from CNFs with \texttt{PySDD}. During each iteration they are used to calculate marginals based on current values of the parameters. Their structure never changes, only their weights are updated as the algorithm progresses. Either because the new estimates have to be taken into account at the end of every iteration, or because the marginals have to be calculated (which is done by toying with the weights rather than by applying Bayes' theorem which was the approach used in section 1).\\

\par\noindent The algorithm is reasonably fast and some results are shown below :

\begin{table}[h]
\centering
\begin{tabular}{ccc}
& &\\\hline
\end{tabular}
\caption{Results of the parameter learning algorithm. Initial weights were always set to 0.5 for the sake of reproducibility (setting them randomly is a matter of commenting out a line). On a regular computer running for 1000 iterations on 1000 examples took no longer than a minute. Various tests were done such as comparing with ProbLog's own system, using the same initial values. }
\label{plres}
\end{table}