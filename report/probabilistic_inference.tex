\begin{multicols*}{2}
\raggedcolumns

\fakesection{Probabilistic Inference Using Weighted Model Counting}

%%%
%%%
%%%

\fakesubsection{SRL to CNF}

First the program is grounded. This is a matter of collecting all atoms involved in all proofs of the query.

\begin{code}
\begin{minted}[xleftmargin=20pt,linenos]{PROLOG}
0.2::stress(a).
0.2::stress(b).
0.2::stress(c).

0.1::friends(a,a).
0.1::friends(a,b).
0.1::friends(a,c).

0.1::friends(b,a).
0.1::friends(b,b).
0.1::friends(b,c).

0.1::friends(c,a).
0.1::friends(c,b).
0.1::friends(c,c).

0.3::smokes(a) :- stress(a).
0.3::smokes(b) :- stress(b).
0.3::smokes(c) :- stress(c).
0.4::smokes(a) :- friends(a,a), smokes(a).
0.4::smokes(a) :- friends(a,b), smokes(b).
0.4::smokes(a) :- friends(a,c), smokes(c).
0.4::smokes(b) :- friends(b,a), smokes(a).
0.4::smokes(b) :- friends(b,b), smokes(b).
0.4::smokes(b) :- friends(b,c), smokes(c).

0.4::smokes(c) :- friends(c,a), smokes(a).
0.4::smokes(c) :- friends(c,b), smokes(b).
0.4::smokes(c) :- friends(c,c), smokes(c).
\end{minted}
\captionof{listing}{Relevant ground program.}
\label{code:base}
\vspace{0.5cm}
\end{code}

\noindent The proofs of the query make for a trie as shown in figure \ref{fig:nestedtries}, where colourings indicate the presence of cycles. Any proof involving an atom \texttt{friends(X,X)} or \texttt{friends(Y,a)} (with $Y\in\{b,c\}$) is non-minimal and doesn't affect the final probability. These atoms are disregarded. For the remaining cycles (involving \texttt{friends(b,c)} and \texttt{friends(c,b)}) auxiliary variables can be used to obtain a cycle-free program without intensional probabilistic facts :

\begin{code}
\begin{minted}[xleftmargin=20pt,linenos]{PROLOG}
0.2::stress(a).
0.2::stress(b).
0.2::stress(c).

0.1::friends(a,b).
0.1::friends(a,c).
0.1::friends(b,c).
0.1::friends(c,b).

0.3::p(a).
0.3::p(b).
0.3::p(c).

0.4::p(a,b).
0.4::p(a,c).
0.4::p(b,c).
0.4::p(c,b).

smokes(a) :- stress(a), p(a).
smokes(b) :- stress(b), p(b).
smokes(c) :- stress(c), p(c).

smokes(a) :- 
    friends(a,b), smokes(b), p(a,b).
smokes(a) :-
    friends(a,c), smokes(c), p(a,c).
smokes(b) :- 
    friends(b,c), stress(c), p(c), p(b,c).
smokes(c) :- 
    friends(c,b), stress(b), p(b), p(c,b).

query(smokes(a)).
\end{minted}
\captionof{listing}{Relevant ground program without cycles.}
\label{code:base}
\vspace{0.5cm}
\end{code}

\noindent The above logic program is equivalent to the following propositional formula :
\begin{center}
\begin{tabular}{ll}
$(smokes(a)\leftrightarrow$ & $(stress(a) \land p(a))$\\
\multicolumn{2}{c}{$\lor\ (friends(a,b) \land smokes(b) \land p(a,b))$}\\
\multicolumn{2}{c}{$\lor\ (friends(a,c) \land smokes(c) \land p(a,c)))$}\\
\multicolumn{2}{c}{$\land$}\\
$(smokes(b)\leftrightarrow$ & $(stress(b) \land p(b))$\\
\multicolumn{2}{c}{$\lor\ (friends(b,c) \land stress(c) \land p(c) \land p(b,c)))$}\\
\multicolumn{2}{c}{$\land$}\\
$(smokes(c)\leftrightarrow$ & $(stress(c) \land p(c))$\\
\multicolumn{2}{c}{$\lor\ (friends(c,b) \land stress(b) \land p(b) \land p(c,b)))$}\\
\end{tabular}
\end{center}

\vspace{0.1cm}
\par\noindent Which corresponds to the following \texttt{CNF} :\vspace{0.2cm}\\
$
(\lnot smokes(a) \lor stress(a) \lor friends(a,b) \lor friends(a,c)) 
\\\land (\lnot smokes(a) \lor stress(a) \lor friends(a,b) \lor smokes(c)) 
\\\land (\lnot smokes(a) \lor stress(a) \lor friends(a,b) \lor p(a,c)) 
\\\land (\lnot smokes(a) \lor stress(a) \lor smokes(b) \lor friends(a,c)) 
\\\land (\lnot smokes(a) \lor stress(a) \lor smokes(b) \lor smokes(c)) 
\\\land (\lnot smokes(a) \lor stress(a) \lor smokes(b) \lor p(a,c)) 
\\\land (\lnot smokes(a) \lor stress(a) \lor p(a,b) \lor friends(a,c)) 
\\\land (\lnot smokes(a) \lor stress(a) \lor p(a,b) \lor smokes(c)) 
\\\land (\lnot smokes(a) \lor stress(a) \lor p(a,b) \lor p(a,c)) 
\\\land (\lnot smokes(a) \lor p(a) \lor friends(a,b) \lor friends(a,c)) 
\\\land (\lnot smokes(a) \lor p(a) \lor friends(a,b) \lor smokes(c)) 
\\\land (\lnot smokes(a) \lor p(a) \lor friends(a,b) \lor p(a,c)) 
\\\land (\lnot smokes(a) \lor p(a) \lor smokes(b) \lor friends(a,c)) 
\\\land (\lnot smokes(a) \lor p(a) \lor smokes(b) \lor smokes(c)) 
\\\land (\lnot smokes(a) \lor p(a) \lor smokes(b) \lor p(a,c)) 
\\\land (\lnot smokes(a) \lor p(a) \lor p(a,b) \lor friends(a,c)) 
\\\land (\lnot smokes(a) \lor p(a) \lor p(a,b) \lor smokes(c)) 
\\\land (\lnot smokes(a) \lor p(a) \lor p(a,b) \lor p(a,c)) 
\\\land (\lnot stress(a) \lor \lnot p(a) \lor smokes(a)) 
\\\land (\lnot friends(a,b) \lor \lnot smokes(b) \lor \lnot p(a,b) \lor smokes(a)) 
\\\land (\lnot friends(a,c) \lor \lnot smokes(c) \lor \lnot p(a,c) \lor smokes(a)) 
\\\land (\lnot smokes(b) \lor stress(b) \lor friends(b,c)) 
\\\land (\lnot smokes(b) \lor stress(b) \lor stress(c)) 
\\\land (\lnot smokes(b) \lor stress(b) \lor p(c)) 
\\\land (\lnot smokes(b) \lor stress(b) \lor p(b,c) 
\\\land (\lnot smokes(b) \lor p(b) \lor friends(b,c)) 
\\\land (\lnot smokes(b) \lor p(b) \lor stress(c)) 
\\\land (\lnot smokes(b) \lor p(b) \lor p(c)) 
\\\land (\lnot smokes(b) \lor p(b) \lor p(b,c) 
\\\land (\lnot stress(b) \lor \lnot p(b) \lor smokes(b)) 
\\\land (\lnot friends(b,c) \lor \lnot stress(c) \lor \lnot p(c) \lor \lnot p(b,c) \lor smokes(b)) 
\\\land (\lnot smokes(c) \lor stress(c) \lor friends(c,b)) 
\\\land (\lnot smokes(c) \lor stress(c) \lor stress(b)) 
\\\land (\lnot smokes(c) \lor stress(c) \lor p(b)) 
\\\land (\lnot smokes(c) \lor stress(c) \lor p(c,b)) 
\\\land (\lnot smokes(c) \lor p(c) \lor friends(c,b)) 
\\\land (\lnot smokes(c) \lor p(c) \lor stress(b)) 
\\\land (\lnot smokes(c) \lor p(c) \lor p(b)) 
\\\land (\lnot smokes(c) \lor p(c) \lor p(c,b)) 
\\\land (\lnot stress(c) \lor \lnot p(c) \lor smokes(c)) 
\\\land (\lnot friends(c,b) \lor \lnot stress(b) \lor \lnot p(b) \lor \lnot p(c,b) \lor smokes(c))
$
\vspace{0.3cm}

\par\noindent The probabilistic literals in the \texttt{CNF} are assigned weights (derived literals get a weight of 1) :

\begin{center}
\begin{tabular}{c|c}
Literal & Weight \\
\hline
stress(a) & 0.2 \\
$\lnot$stress(a) & 0.8 \\
stress(b) & 0.2 \\
$\lnot$stress(b) & 0.8 \\
stress(c) & 0.2 \\
$\lnot$stress(c) & 0.8 \\
friends(a,b) & 0.1 \\
$\lnot$friends(a,b) & 0.9 \\
friends(a,c) & 0.1 \\
$\lnot$friends(a,c) & 0.9 \\
friends(b,c) & 0.1 \\
$\lnot$friends(b,c) & 0.9 \\
friends(c,b) & 0.1 \\
$\lnot$friends(c,b) & 0.9 \\
p(a) & 0.3 \\
$\lnot$p(a) & 0.7 \\
p(b) & 0.3 \\
$\lnot$p(b) & 0.7 \\
p(c) & 0.3 \\
$\lnot$p(c) & 0.7 \\
p(a,b) & 0.4 \\
$\lnot$p(a,b) & 0.6 \\
p(a,c) & 0.4 \\
$\lnot$p(a,c) & 0.6 \\
p(b,c) & 0.4 \\
$\lnot$p(b,c) & 0.6 \\
p(c,b) & 0.4 \\
$\lnot$p(c,b) & 0.6 \\
\end{tabular}
\end{center}

\definecolor{darkgray}{rgb}{0.4,0.4,0.4}
\definecolor{c1}{rgb}{0.83,0.13,0.18}
\definecolor{c2}{rgb}{0.23,0.48,0.34}
\definecolor{c3}{rgb}{0.18,0.35,0.58}
\begin{figure*}
\centering
\begin{tikzpicture}[scale=0.85]

	\node at (0,0) (1) {\texttt{\textcolor{c1}{smokes(a)}}};
	
	\node at (0,-2) (2) {\texttt{stress(a)}};
	\node at (0,-4) (6) {\texttt{stress(b)}};
	\node at (0,-6) (7) {\texttt{stress(c)}};
	
	\node at (4,-2) (3) {\texttt{friends(a,a),\textcolor{c1}{smokes(a)}}};
	\node at (8,-3) (4) {\texttt{friends(a,b),\textcolor{c2}{smokes(b)}}};
	\node at (12,-4) (5) {\texttt{friends(a,c),\textcolor{c3}{smokes(c)}}};
	
	\node at (4,-5) (8) {\texttt{friends(b,a),\textcolor{c1}{smokes(a)}}};
	\node at (4,-6) (9) {\texttt{friends(b,b),\textcolor{c2}{smokes(b)}}};
	\node at (4,-7) (10) {\texttt{friends(b,c),\textcolor{c3}{smokes(c)}}};
	
	\node at (7,-8) (11) {\texttt{friends(c,a),\textcolor{c1}{smokes(a)}}};
	\node at (7,-9) (12) {\texttt{friends(c,b),\textcolor{c2}{smokes(b)}}};
	\node at (7,-10) (13) {\texttt{friends(c,c),\textcolor{c3}{smokes(c)}}};
	
	\draw[darkgray,->] (1.south) to (2.north);
	
	\draw[darkgray,->] (1.east) to [out=0,in=90,looseness=0.9](3.north);
	\draw[darkgray,->] (1.east) to [out=0,in=90,looseness=0.9](4.north);
	\draw[darkgray,->] (1.east) to [out=0,in=90,looseness=0.9](5.north);
	
	\draw[darkgray,->] (4.west) to [out=180,in=90,looseness=0.8](6.north);
	\draw[darkgray,->] (5.west) to [out=180,in=90,looseness=0.6](7.north);
	
	\draw[darkgray,->] ([xshift=6mm]4.south) to [out=-90,in=0,looseness=0.9](8.east);
	\draw[darkgray,->] ([xshift=6mm]4.south) to [out=-90,in=0,looseness=0.9](9.east);
	\draw[darkgray,->] ([xshift=6mm]4.south) to [out=-90,in=0,looseness=0.9](10.east);
	
	\draw[darkgray,->] ([xshift=6mm]5.south) to [out=-90,in=0,looseness=0.9](11.east);
	\draw[darkgray,->] ([xshift=6mm]5.south) to [out=-90,in=0,looseness=0.9](12.east);
	\draw[darkgray,->] ([xshift=6mm]5.south) to [out=-90,in=0,looseness=0.9](13.east);
	
	% Loops
	
	%\draw[->,dashed,blue] ([xshift=-4mm]3.north) to ([xshift=4mm]1.south);
%	
%	\draw[->,dashed,blue] ([xshift=-4mm]8.north) to ([xshift=4mm,looseness=0.5]1.south);
%	\draw[->,dashed,blue] ([xshift=-4mm]9.east) to ([xshift=4mm,looseness=0.5]4.south);
%	\draw[->,dashed,blue] ([xshift=-4mm]10.east) to ([xshift=4mm,looseness=0.5]5.south);
%	
%	\draw[->,dashed,blue] ([xshift=-4mm]11.east) to ([xshift=4mm,looseness=0.5]1.south);
%	\draw[->,dashed,blue] ([xshift=-4mm]12.east) to ([xshift=4mm,looseness=0.5]4.south);
%	\draw[->,dashed,blue] ([xshift=-4mm]13.east) to ([xshift=4mm,looseness=0.5]5.south);
	
%	\node at (-2.5,5) (3) {$\mathbb{N}$};
%	\draw[->,shorten >=1pt] (3) to [out=180,in=-90,loop,looseness=15] node[left]{$id$} (3);
%	\draw[->,shorten >=1pt] (3) to [out=180,in=-90,loop,looseness=30] node[left]{$add1mod3$} (3);
%	\draw[->,shorten >=1pt] (3) to [out=180,in=-90,loop,looseness=45] node[left]{$add2mod3$} (3);
\end{tikzpicture}
\caption{Trie representing proofs of the query. Coloured atoms indicate the presence of cycles.}
\label{fig:nestedtries}
\end{figure*}

%%%
%%%
%%%

\fakesubsection{SRL to PGM}

\begin{figure*}
	\centering
	\begin{tikzpicture}[scale=0.85, every node/.style={scale=0.85}]
	
		% Nodes
		\node[obs] (sma) {$smokes(a)$};
		\node[latent,below=of sma,xshift=-8cm] (aux1) {$aux_1$};
		\node[latent,below=of aux1] (sta) {$stress(a)$};
		\node[latent,below=of sma,xshift=-2cm] (aux6) {$aux_6$};
		\node[latent,below=of sma,xshift=4cm] (aux7) {$aux_7$};
		\node[latent,below=of aux6] (smb) {$smokes(b)$};
		\node[latent,below=of aux7] (smc) {$smokes(c)$};
		\node[latent,below=of aux6,xshift=-3cm] (friendsab) {$friends(a,b)$};
		\node[latent,below=of aux7,xshift=3cm] (friendsac) {$friends(a,c)$};
		\node[latent,below=of smb,xshift=2cm] (aux2) {$aux_2$};
		\node[latent,below=of smc,xshift=2cm] (aux3) {$aux_3$};
		\node[latent,below=of smb,xshift=-2cm] (aux4) {$aux_4$};
		\node[latent,below=of smc,xshift=-2cm] (aux5) {$aux_5$};
		\node[latent,below=of aux2,xshift=-0.5cm] (stb) {$stress(b)$};
		\node[latent,below=of aux3,xshift=0cm] (stc) {$stress(c)$};
		\node[latent,below=of aux4,xshift=0.5cm] (friendsbc) {$friends(b,c)$};
		\node[latent,below=of aux5,xshift=1cm] (friendscb) {$friends(c,b)$};
%		\node[latent] (pa) {$p(a)$};
%		\node[latent] (pb) {$p(b)$};
%		\node[latent] (pc) {$p(c)$};
%		\node[latent] (pab) {$p(a,b)$};
%		\node[latent] (pac) {$p(a,c)$};
%		\node[latent] (pbc) {$p(b,c)$};
%		\node[latent] (pcb) {$p(c,b)$};
		
		% Edges
		\edge {aux1,aux6,aux7} {sma}
		\edge {aux2,aux4} {smb}
		\edge {aux3,aux5} {smc}
		\edge {sta} {aux1}
		\edge {stb} {aux2}
		\edge {stc} {aux3}
		\edge {friendsbc} {aux4}
		%\edge {stc} {aux4}
		\edge {friendscb,aux2} {aux5}
		\edge {friendsab,smb} {aux6}
		\edge {friendsac,smc} {aux7}
		\draw[->,shorten >=1pt] (aux3) to [out=-50,in=225,looseness=2] node[left]{$$} (aux4);

		% Plates
		% \plate [inner sep=.25cm,yshift=.2cm] {plate1} {(x)(y)(z)} {$N$}; %

\end{tikzpicture}
\caption{Bayesian network corresponding to the ground acyclic program.}
\label{fig:bayesian}
\end{figure*}

A Bayesian network is shown in figure \ref{fig:bayesian}. The conditional probability tables (CPTs) for the nodes are given below. Note that every table represents multiple identical tables in the network (like those of the $stress(a)$, $stress(b)$ and $stress(c)$ nodes for example).

\begin{center}

	% Stress, Friends
	\begin{tabular}{ccccc}
		\multicolumn{2}{c}{\underline{$stress(\{a,b,c\})$}} & & \multicolumn{2}{c}{\underline{$friends(\{a,b,c\},\{b,c\})$}}\\
		$\top$ & 0.2 & & $\top$ & 0.1\\
		%\hline
		$\bot$ & 0.8 & & $\bot$ & 0.9 \\
	\end{tabular}
	\vspace{0.5cm}\\
	
	% Auxiliary 1,2,3
	\begin{tabular}{c|cc}
		\underline{$stress(\{a,b,c\})$} & \multicolumn{2}{c}{\underline{$aux_{\{1,2,3\}}$}} \\
		& $\top$ & $\bot$ \\
		%\hline
		$\top$ & 0.3 & 0.7 \\
		$\bot$ & 0.0 & 1.0
	\end{tabular}
	\vspace{0.5cm}\\
	
	% Auxiliary 4,5
	\begin{tabular}{cc|cc}
		\underline{$friends(\{b,c\},\{c,b\})$} & \underline{$aux_{2,3}$} & \multicolumn{2}{c}{\underline{$aux_{4,5}$}} \\
		& & $\top$ & $\bot$ \\
		%\hline
		$\top$ & $\top$ & 0.4 & 0.6 \\
		$\top$ & $\bot$ & 0.0 & 1.0 \\
		$\bot$ & $\top$ & 0.0 & 1.0 \\
		$\bot$ & $\bot$ & 0.0 & 1.0 \\
	\end{tabular}
	\vspace{0.5cm}\\
	
	% Auxiliary 6,7
	\begin{tabular}{cc|cc}
		\underline{$friends(\{a\},\{b,c\})$} & \underline{$smokes(\{b,c\})$} & \multicolumn{2}{c}{\underline{$aux_{6,7}$}} \\
		& & $\top$ & $\bot$ \\
		%\hline
		$\top$ & $\top$ & 0.4 & 0.6 \\
		$\top$ & $\bot$ & 0.0 & 1.0 \\
		$\bot$ & $\top$ & 0.0 & 1.0 \\
		$\bot$ & $\bot$ & 0.0 & 1.0 \\
	\end{tabular}
	\vspace{0.5cm}\\
	
	% Smokes b,c
	\begin{tabular}{cc|cc}
		\underline{$aux_{4,5}$} & \underline{$aux_{2,3}$} & \multicolumn{2}{c}{\underline{$smokes(\{b,c\})$}} \\
		& & $\top$ & $\bot$ \\
		%\hline
		$\top$ & $\top$ & 1.0 & 0.0 \\
		$\top$ & $\bot$ & 0.0 & 1.0 \\
		$\bot$ & $\top$ & 0.0 & 1.0 \\
		$\bot$ & $\bot$ & 0.0 & 1.0 \\
	\end{tabular}
	\vspace{0.5cm}\\
	
	% Smokes b,c
	\begin{tabular}{ccc|cc}
		\underline{$aux_{1}$} & \underline{$smokes(b)$} & \underline{$smokes(c)$} & \multicolumn{2}{c}{\underline{$smokes(a)$}} \\
		& & & $\top$ & $\bot$ \\
		%\hline
		$\top$ & $\top$ & $\top$ & 1.0 & 0.0 \\
		$\top$ & $\top$ & $\bot$ & 0.0 & 1.0 \\
		$\top$ & $\bot$ & $\top$ & 0.0 & 1.0 \\
		$\top$ & $\bot$ & $\bot$ & 0.0 & 1.0 \\
		$\bot$ & $\top$ & $\top$ & 0.0 & 1.0 \\
		$\bot$ & $\top$ & $\bot$ & 0.0 & 1.0 \\
		$\bot$ & $\bot$ & $\top$ & 0.0 & 1.0 \\
		$\bot$ & $\bot$ & $\bot$ & 0.0 & 1.0 \\
	\end{tabular}
	\vspace{0.5cm}

\end{center}

%%%
%%%
%%%

\fakesubsection{PGM to CNF}

The Bayesian network can be encoded as a logical formula. Encodings ENC1 and ENC2 discussed by Chavira \cite{chavira} both make use of the same indicator variables such as $\lambda_{stress(a)=true}$ and $\lambda_{stress(a)=false}$ (which introduces redundancy considering the fact that all network variables are boolean). \\

\noindent In \texttt{ENC1} each row in each CPT is encoded by a parameter clause.\\
\noindent In \texttt{ENC2} an order is assumed over each variable's values. Then, each row of each CPT is encoded by an equivalence.\\

\noindent For both of these a Python script was written to automate the creation of CNF files, since manual conversion to a \texttt{CNF} turned out to be cumbersome. The script also produces \LaTeX output which is added to the appendix.\\

\noindent Since the noisy OR relations end up being encoded naively by considering each row of the CPT separately, a more compact encoding is obtained by replacing this part of the encoding with a one-liner. For example :
$$smokes(a)\Leftrightarrow aux_1 \lor aux_6 \lor aux_7$$
\noindent This can be converted to a \texttt{CNF} in the usual way :
\begin{gather*}
(smokes(a) \lor \lnot aux_1\lor\lnot aux_6\lor\lnot aux_7)\\
\land\ (\lnot smokes(a) \lor aux_1)\\
\land\ (\lnot smokes(a) \lor aux_6)\\
\land\ (\lnot smokes(a) \lor aux_7)
\end{gather*}

%%%
%%%
%%%

\fakesubsection{Weighted Model Counting}

The previous paragraphs led to the construction of 5 different encodings. All of them lie in the \texttt{/src/bayesian/} folder, having the following file names :
\begin{center}
\begin{tabular}{cc}
Encoding & Filename \\\hline
Conversion from ProbLog & \texttt{propositional.cnf}\\
\texttt{ENC1} & \texttt{enc1.cnf}\\
\texttt{ENC1} + noisy OR & \texttt{enc1\_noisy.cnf}\\
\texttt{ENC2} & \texttt{enc2.cnf}\\
\texttt{ENC2} + noisy OR & \texttt{enc2\_noisy.cnf}\\
\end{tabular}
\end{center}
\noindent Weighted model counting was performed on all of them using either \texttt{PySDD} or \texttt{miniC2D}. A modified version of \texttt{Cachet} that allows for the specification of negative weights was toyed with too, though not for all encodings as it expects a different format for specifying the weights. These were the results :
\begin{center}
\begin{tabular}{c|cc}
Encoding & \texttt{PySDD} & \texttt{miniC2D} \\\hline
Conversion from ProbLog & XXX & XXX\\
\texttt{ENC1} & XXX & XXX\\
\texttt{ENC1} + noisy OR & XXX & XXX\\
\texttt{ENC2} & XXX & XXX\\
\texttt{ENC2} + noisy OR & XXX & XXX\\
\end{tabular}
\end{center}

\noindent The counts can be interpreted as probabilities (keeping in mind that some assumptions result from the semantics of the ProbLog program, such as independence of friendships). In this particular case since no query was specified (yet), the probability equals 1 since the count represents the probability of any possible world.\\

\par\noindent To answer a more complex query like $P(smokes(a)=\top)$ a line was added to the CNF files : $smokes(a)$.  This makes sure that any model is one where person $a$ actually smokes. A more complex query like $P(smokes(a)=\top\ |\ friends(a,b)=\top,friends(a,c)=\top)$ can be computed in the way Fierens proposed \cite{fierens}, by dividing the joint probability of these three atoms by the probability of the previous query. One could also toy with the weights of the literals. 

\end{multicols*}

\par\noindent The smallest circuit sizes were the following : 
\begin{center}
\begin{tabular}{cc|cc}
Encoding & Hyperparameters & \texttt{PySDD} & \texttt{miniC2D} \\\hline
Conversion from ProbLog & XXX & XXX & XXX\\
\texttt{ENC1} & XXX & XXX & XXX\\
\texttt{ENC1} + noisy OR & XXX & XXX & XXX\\
\texttt{ENC2} & XXX & XXX & XXX\\
\texttt{ENC2} + noisy OR & XXX & XXX & XXX\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{cc|cc}
Encoding & $P(smokes(a)=\top)$ & $P(smokes(a)=\top\ |\ friends(a,b)=\top,friends(a,c)=\top)$ \\\hline
Conversion from ProbLog & XXX & XXX\\
\texttt{ENC1} & XXX & XXX\\
\texttt{ENC1} + noisy OR & XXX & XXX\\
\texttt{ENC2} & XXX & XXX\\
\texttt{ENC2} + noisy OR & XXX & XXX\\
\end{tabular}
\end{center}

\par\noindent EXPLAIN DIFFERENCE BETWEEN MODEL COUNTERS HERE

